{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D655LaTpQSx1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def polynomial_regression(degree, X, y, folds, test_size=0.25, random_state=None):\n",
        "    # Define number of folds for cross-validation\n",
        "    kf = KFold(folds)\n",
        "\n",
        "    # Initialize lists to store results for variance, bias2s, total_error, models, r2, and mse\n",
        "    variance = []\n",
        "    bias2 = []\n",
        "    total_error = []\n",
        "    models = []\n",
        "    r2 = []\n",
        "    mse = []\n",
        "\n",
        "    # Set the polynomial degree of the model\n",
        "    poly_features = PolynomialFeatures(degree)\n",
        "    X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "    # Perform cross-validation\n",
        "    for train_index, test_index in kf.split(X_poly):\n",
        "        # Split data into training and testing sets for this fold\n",
        "        X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Fit polynomial regression model\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate variance and bias for this fold\n",
        "        variance_fold = np.var(y_pred)\n",
        "        bias2_fold = np.mean((np.mean(y_pred) - y_test) ** 2) #𝐵𝑖𝑎𝑠2 = 𝐸[( 𝐸[𝑔(𝑥)] − 𝑓(𝑥) )^2 ]\n",
        "        total_error_fold = variance_fold + bias2_fold #𝐸𝑟𝑟𝑜𝑟 𝑜𝑓 𝑡ℎ𝑒 𝑚𝑜𝑑𝑒𝑙 = 𝐵𝑖𝑎𝑠2 + 𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒 + 𝐼𝑟𝑟𝑒𝑑𝑢𝑐𝑖𝑏𝑙𝑒 𝐸𝑟𝑟𝑜𝑟\n",
        "\n",
        "        # Calculate R2 and MSE for this fold\n",
        "        r2_fold = r2_score(y_test, y_pred)\n",
        "        mse_fold = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "        # Append results to lists\n",
        "        variance.append(variance_fold)\n",
        "        bias2.append(bias2_fold)\n",
        "        total_error.append(total_error_fold)\n",
        "        models.append(model)\n",
        "\n",
        "        # Print results for this fold\n",
        "        print(\"Variance: {:.4f}, Bias2: {:.4f}, Total error: {:.4f}, R^2: {:.4f}, MSE: {:.4f}\".format(variance_fold, bias2_fold, total_error_fold, r2_fold, mse_fold))\n",
        "\n",
        "    # print the total_error of the best model\n",
        "    min_error_index = np.argmin(total_error)\n",
        "    best_model = models[min_error_index]\n",
        "    print(\"Total error of the best model: {:.4f}\".format(total_error[min_error_index]))\n",
        "\n",
        "    # Testing the final model on the test data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=test_size, random_state=42)\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Store mse and r2 score of the model applied on the test data\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "    test_r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"Test MSE: {:.4f}\".format(test_mse))\n",
        "    print(\"Test R^2: {:.4f}\".format(test_r2))\n",
        "\n",
        "    return test_mse, best_model, total_error[min_error_index] # Can be modified to return whatever we need"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "california_housing = fetch_california_housing()\n",
        "# print the attributes of the dataset\n",
        "# X = california_housing.data[:, [1, 3, 4]]\n",
        "X = california_housing.data[:, [0, 2, 3]]\n",
        "y = california_housing.target\n",
        "\n",
        "# print the 0th attribute of the dataset\n",
        "print(X[:, 0])\n",
        "print(california_housing.data[:, 0])\n",
        "\n",
        "\n",
        "mse_list = []\n",
        "best_model_list = []\n",
        "te_list = []\n",
        "\n",
        "degrees = range(1, 6)  # Try degrees from 1 to 5\n",
        "for degree in degrees:\n",
        "    mse , best_model , te = polynomial_regression(degree=degree, X=X, y=y, folds = 5 ,test_size=0.25, random_state=42 )\n",
        "    print(\"Degree:\", degree, \"MSE:\", mse)\n",
        "    print(\"\\n\")\n",
        "    mse_list.append(mse)\n",
        "    best_model_list.append(best_model)\n",
        "    te_list.append(te)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caOYPi8WWjgb",
        "outputId": "3ec6a35a-1b18-4e41-88b4-10f4f2e92cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8.3252 8.3014 7.2574 ... 1.7    1.8672 2.3886]\n",
            "[8.3252 8.3014 7.2574 ... 1.7    1.8672 2.3886]\n",
            "Variance: 0.6159, Bias2: 1.1592, Total error: 1.7751, R^2: 0.4923, MSE: 0.5454\n",
            "Variance: 0.6857, Bias2: 1.2147, Total error: 1.9004, R^2: 0.4238, MSE: 0.6744\n",
            "Variance: 0.8001, Bias2: 1.4449, Total error: 2.2450, R^2: 0.4928, MSE: 0.7296\n",
            "Variance: 0.4341, Bias2: 1.1848, Total error: 1.6190, R^2: 0.3705, MSE: 0.7385\n",
            "Variance: 0.7587, Bias2: 1.4722, Total error: 2.2309, R^2: 0.5437, MSE: 0.6649\n",
            "Total error of the best model: 1.6190\n",
            "Test MSE: 0.6592\n",
            "Test R^2: 0.5018\n",
            "Degree: 1 MSE: 0.6592342970938996\n",
            "\n",
            "\n",
            "Variance: 1.4212, Bias2: 1.1390, Total error: 2.5602, R^2: 0.0108, MSE: 1.0627\n",
            "Variance: 0.6013, Bias2: 1.1999, Total error: 1.8012, R^2: 0.4689, MSE: 0.6217\n",
            "Variance: 0.8027, Bias2: 1.4435, Total error: 2.2462, R^2: 0.5090, MSE: 0.7064\n",
            "Variance: 0.5257, Bias2: 1.1828, Total error: 1.7085, R^2: 0.3850, MSE: 0.7215\n",
            "Variance: 0.8059, Bias2: 1.4748, Total error: 2.2808, R^2: 0.5672, MSE: 0.6307\n",
            "Total error of the best model: 1.7085\n",
            "Test MSE: 0.6243\n",
            "Test R^2: 0.5282\n",
            "Degree: 2 MSE: 0.6243291681252873\n",
            "\n",
            "\n",
            "Variance: 11.4957, Bias2: 1.1280, Total error: 12.6237, R^2: -9.7176, MSE: 11.5136\n",
            "Variance: 0.5895, Bias2: 1.1955, Total error: 1.7849, R^2: 0.4803, MSE: 0.6084\n",
            "Variance: 0.9781, Bias2: 1.4401, Total error: 2.4183, R^2: 0.4527, MSE: 0.7873\n",
            "Variance: 0.5374, Bias2: 1.1802, Total error: 1.7175, R^2: 0.3923, MSE: 0.7130\n",
            "Variance: 0.8490, Bias2: 1.4749, Total error: 2.3240, R^2: 0.5871, MSE: 0.6016\n",
            "Total error of the best model: 1.7175\n",
            "Test MSE: 0.6087\n",
            "Test R^2: 0.5400\n",
            "Degree: 3 MSE: 0.6086722563613092\n",
            "\n",
            "\n",
            "Variance: 1749.0026, Bias2: 1.3864, Total error: 1750.3890, R^2: -1628.3416, MSE: 1750.3564\n",
            "Variance: 0.5950, Bias2: 1.1899, Total error: 1.7849, R^2: 0.4839, MSE: 0.6041\n",
            "Variance: 1.0413, Bias2: 1.4397, Total error: 2.4810, R^2: 0.4270, MSE: 0.8243\n",
            "Variance: 0.5600, Bias2: 1.1775, Total error: 1.7376, R^2: 0.4058, MSE: 0.6971\n",
            "Variance: 0.8850, Bias2: 1.4750, Total error: 2.3600, R^2: 0.5813, MSE: 0.6101\n",
            "Total error of the best model: 1.7376\n",
            "Test MSE: 0.5971\n",
            "Test R^2: 0.5487\n",
            "Degree: 4 MSE: 0.597114217354233\n",
            "\n",
            "\n",
            "Variance: 421853.5240, Bias2: 189.4010, Total error: 422042.9250, R^2: -392805.2508, MSE: 421980.8581\n",
            "Variance: 0.6067, Bias2: 1.1883, Total error: 1.7950, R^2: 0.4843, MSE: 0.6037\n",
            "Variance: 1.0282, Bias2: 1.4402, Total error: 2.4684, R^2: 0.4480, MSE: 0.7941\n",
            "Variance: 1.1646, Bias2: 1.1801, Total error: 2.3447, R^2: -0.1191, MSE: 1.3130\n",
            "Variance: 0.8897, Bias2: 1.4742, Total error: 2.3638, R^2: 0.5823, MSE: 0.6086\n",
            "Total error of the best model: 1.7950\n",
            "Test MSE: 0.5902\n",
            "Test R^2: 0.5539\n",
            "Degree: 5 MSE: 0.5902279219433645\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}